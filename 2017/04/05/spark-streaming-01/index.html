<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.9.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="Spark源码-streaming-01-Job 生成">




  <meta name="keywords" content="spark,">





  <link rel="alternate" href="/default" title="爱吃柚子">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=1.1">



<link rel="canonical" href="http://acyouzi.com/2017/04/05/spark-streaming-01/">


<meta name="description" content="总结 在看代码之前一直以为 Dstream 继承自 RDD, 原来他们之间并没有继承关系，DStream 是在 jobGenerator 时才创建了计算所需的 RDD。  发现一个 github 项目超级棒，讲 Streaming 源码的，看源码的时候可以对照那里面的文章，地址是：  https://github.com/lw-lin/CoolplaySpark   流程 JobGenerator">
<meta name="keywords" content="spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark源码-streaming-01-Job 生成">
<meta property="og:url" content="http://acyouzi.com/2017/04/05/spark-streaming-01/index.html">
<meta property="og:site_name" content="爱吃柚子">
<meta property="og:description" content="总结 在看代码之前一直以为 Dstream 继承自 RDD, 原来他们之间并没有继承关系，DStream 是在 jobGenerator 时才创建了计算所需的 RDD。  发现一个 github 项目超级棒，讲 Streaming 源码的，看源码的时候可以对照那里面的文章，地址是：  https://github.com/lw-lin/CoolplaySpark   流程 JobGenerator">
<meta property="og:locale" content="zh">
<meta property="og:updated_time" content="2019-08-07T13:56:51.152Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark源码-streaming-01-Job 生成">
<meta name="twitter:description" content="总结 在看代码之前一直以为 Dstream 继承自 RDD, 原来他们之间并没有继承关系，DStream 是在 jobGenerator 时才创建了计算所需的 RDD。  发现一个 github 项目超级棒，讲 Streaming 源码的，看源码的时候可以对照那里面的文章，地址是：  https://github.com/lw-lin/CoolplaySpark   流程 JobGenerator">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d310e8dddd6fde3a70c9b10c17107ae3";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-89126170-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-89126170-1');
  </script>


  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- 方框 -->
  <ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4100561021751428" data-ad-slot="1154304039" data-ad-format="auto" data-full-width-responsive="true"></ins>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
  </script>



    <title> Spark源码-streaming-01-Job 生成 - 爱吃柚子 </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">爱吃柚子</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="/archives">
                            
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          Spark源码-streaming-01-Job 生成
        
      </h1>

      <time class="post-time">
          Apr 05 2017
      </time>
    </header>
    
            <div class="post-content">
            <h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li><p>在看代码之前一直以为 Dstream 继承自 RDD, 原来他们之间并没有继承关系，DStream 是在 jobGenerator 时才创建了计算所需的 RDD。</p>
</li>
<li><p>发现一个 github 项目超级棒，讲 Streaming 源码的，看源码的时候可以对照那里面的文章，地址是：</p>
<p> <a href="https://github.com/lw-lin/CoolplaySpark" target="_blank" rel="noopener">https://github.com/lw-lin/CoolplaySpark</a></p>
</li>
</ol>
<h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><ol>
<li><p>JobGenerator 的构造方法中创建了一个 RecurringTimer 定时发送 GenerateJobs 消息</p>
<pre><code>private val timer = new RecurringTimer(clock, ssc.graph.batchDuration.milliseconds,
longTime =&gt; eventLoop.post(GenerateJobs(new Time(longTime))), &quot;JobGenerator&quot;)</code></pre><p> 而在 JobGenerator#start 方法中又启动了一个线程来负责处理发送给 JobGenerator 的消息</p>
<pre><code>eventLoop = new EventLoop[JobGeneratorEvent](&quot;JobGenerator&quot;) {
  override protected def onReceive(event: JobGeneratorEvent): Unit = processEvent(event)

  override protected def onError(e: Throwable): Unit = {
    jobScheduler.reportError(&quot;Error in job generator&quot;, e)
  }
}
eventLoop.start()</code></pre></li>
<li><p>下面直接看处理 JobGenerator 消息的具体方法：JobGenerator#generateJobs</p>
<pre><code>private def generateJobs(time: Time) {
  // Checkpoint all RDDs marked for checkpointing to ensure their lineages are
  // truncated periodically. Otherwise, we may run into stack overflows (SPARK-6847).
  ssc.sparkContext.setLocalProperty(RDD.CHECKPOINT_ALL_MARKED_ANCESTORS, &quot;true&quot;)
  Try {
    // 使用底层的 receivedBlockTracker 分配 blocks
    jobScheduler.receiverTracker.allocateBlocksToBatch(time) 
    // 生成 job
    // 下面重点看这个方法
    graph.generateJobs(time)
  } match {
    case Success(jobs) =&gt;
      val streamIdToInputInfos = jobScheduler.inputInfoTracker.getInfo(time)
      // 这个方法会在一个新的线程中调用 job 的 run 方法
      jobScheduler.submitJobSet(JobSet(time, jobs, streamIdToInputInfos))
    case Failure(e) =&gt;
      jobScheduler.reportError(&quot;Error generating jobs for time &quot; + time, e)
      PythonDStream.stopStreamingContextIfPythonProcessIsDead(e)
  }
  eventLoop.post(DoCheckpoint(time, clearCheckpointDataLater = false))
}</code></pre></li>
<li><p>DStreamGraph.generateJobs(time)</p>
<pre><code>def generateJobs(time: Time): Seq[Job] = {
  logDebug(&quot;Generating jobs for time &quot; + time)
  val jobs = this.synchronized {
    // 可以看到有几个 outputStream 就有几个 job
    outputStreams.flatMap { outputStream =&gt;
      // 注意这里调用的是 ForEachDStream.generateJob
      val jobOption = outputStream.generateJob(time)
      jobOption.foreach(_.setCallSite(outputStream.creationSite))
      jobOption
    }
  }
  logDebug(&quot;Generated &quot; + jobs.length + &quot; jobs for time &quot; + time)
  jobs
}</code></pre><p> ForEachDStream.generateJob 中如果这个 rdd 曾经生成过则复用，否则重新计算。</p>
<pre><code>override def generateJob(time: Time): Option[Job] = {
  // 下面看 getOrCompute 方法
  parent.getOrCompute(time) match {
    case Some(rdd) =&gt;
      // 这个函数对应到 RDD 里面应该算是 ResultRDD 的处理函数
      // foreachFunc 是一些基于 ForEachDStream 的算子传入的函数
      val jobFunc = () =&gt; createRDDWithLocalProperties(time, displayInnerRDDOps) {
        foreachFunc(rdd, time)
      }
      Some(new Job(time, jobFunc))
    case None =&gt; None
  }
}</code></pre></li>
<li><p>getOrCompute 方法</p>
<pre><code>private[streaming] final def getOrCompute(time: Time): Option[RDD[T]] = {
  // 如果 RDD 已经生成过了直接使用以前的
  // 否则重新计算
  // generatedRDDs 是一个 HashMap，以 time 为键
  generatedRDDs.get(time).orElse {
    // Compute the RDD if time is valid (e.g. correct time in a sliding window)
    // of RDD generation, else generate nothing.
    if (isTimeValid(time)) {
      val rddOption = createRDDWithLocalProperties(time, displayInnerRDDOps = false) {
        // 不检查输出目录的合法性(目录是否存在？)
        PairRDDFunctions.disableOutputSpecValidation.withValue(true) {
          // 注意前边 ForEachDStream.generateJob 调用的是 parent.getOrCompute
          // 所以这个 compute 应该是 ForEachDStream 上一个依赖的方法
          // compute 有多种实现，后面会看几个
          compute(time)
        }
      }

      rddOption.foreach { case newRDD =&gt;
        // Register the generated RDD for caching and checkpointing
        if (storageLevel != StorageLevel.NONE) {
          newRDD.persist(storageLevel)
          logDebug(s&quot;Persisting RDD ${newRDD.id} for time $time to $storageLevel&quot;)
        }
        if (checkpointDuration != null &amp;&amp; (time - zeroTime).isMultipleOf(checkpointDuration)) {
          newRDD.checkpoint()
          logInfo(s&quot;Marking RDD ${newRDD.id} for time $time for checkpointing&quot;)
        }
        generatedRDDs.put(time, newRDD)
      }
      rddOption
    } else {
      None
    }
  }
}</code></pre></li>
<li><p>compute 方法</p>
<ul>
<li><p>ReceiverInputDStream#compute </p>
<pre><code>// 这个方法拿的是 reciver 接收的数据
// 创建的是 blockRDD
override def compute(validTime: Time): Option[RDD[T]] = {
  val blockRDD = {

    if (validTime &lt; graph.startTime) {
      // If this is called for any time before the start time of the context,
      // then this returns an empty RDD. This may happen when recovering from a
      // driver failure without any write ahead log to recover pre-failure data.
      new BlockRDD[T](ssc.sc, Array.empty)
    } else {
      // Otherwise, ask the tracker for all the blocks that have been allocated to this stream
      // for this batch
      val receiverTracker = ssc.scheduler.receiverTracker
      val blockInfos = receiverTracker.getBlocksOfBatch(validTime).getOrElse(id, Seq.empty)

      // Register the input blocks information into InputInfoTracker
      val inputInfo = StreamInputInfo(id, blockInfos.flatMap(_.numRecords).sum)
      ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)

      // 根据 wal 日志是否开启以及块是否为空分别处理创建 RDD
      createBlockRDD(validTime, blockInfos)
    }
  }
  Some(blockRDD)
}</code></pre></li>
<li><p>ShuffledDStream#compute</p>
<pre><code>override def compute(validTime: Time): Option[RDD[(K, C)]] = {
  // 一般 RDD 的这个方法都是下面的流程，拿到父类的数据，然后处理
  parent.getOrCompute(validTime) match {
    case Some(rdd) =&gt; Some(rdd.combineByKey[C](
        createCombiner, mergeValue, mergeCombiner, partitioner, mapSideCombine))
    case None =&gt; None
  }
}</code></pre></li>
</ul>
</li>
<li><p>拿到 Job 之后就该运行 Job 了, 也就是这条语句 :  jobScheduler.submitJobSet(JobSet(time, jobs, streamIdToInputInfos))</p>
<pre><code>def submitJobSet(jobSet: JobSet) {
  if (jobSet.jobs.isEmpty) {
    logInfo(&quot;No jobs added for time &quot; + jobSet.time)
  } else {
    listenerBus.post(StreamingListenerBatchSubmitted(jobSet.toBatchInfo))
    jobSets.put(jobSet.time, jobSet)
    // jobExecutor 是线程池，默认大小是 1 , 可以通过 spark.streaming.concurrentJobs 设置
    // JobHandler 的 run 方法做了诸多配置，最终调用了 Job 的 run 方法
    jobSet.jobs.foreach(job =&gt; jobExecutor.execute(new JobHandler(job)))
    logInfo(&quot;Added jobs for time &quot; + jobSet.time)
  }
}</code></pre><p> Job#run</p>
<pre><code>def run() {
  // func 就是创建 Job 时传入的 foreachFunc
  _result = Try(func())
}</code></pre></li>
<li><p>那么下一问题是 job 在哪里提交的？DStream 有一些 output 算子, 这些算子会调用 RDD 的 action 算子，然后就能触发 job 提交了。比如 print 方法</p>
<pre><code>def print(num: Int): Unit = ssc.withScope {
  def foreachFunc: (RDD[T], Time) =&gt; Unit = {
    (rdd: RDD[T], time: Time) =&gt; {
      // action 算子
      val firstNum = rdd.take(num + 1)
      // scalastyle:off println
      println(&quot;-------------------------------------------&quot;)
      println(s&quot;Time: $time&quot;)
      println(&quot;-------------------------------------------&quot;)
      firstNum.take(num).foreach(println)
      if (firstNum.length &gt; num) println(&quot;...&quot;)
      println()
      // scalastyle:on println
    }
  }
  foreachRDD(context.sparkContext.clean(foreachFunc), displayInnerRDDOps = false)
}</code></pre></li>
</ol>

            </div>
          

    
      <footer class="post-footer">
		
		<div class="post-tags">
		  
			<a href="/tags/spark/">spark</a>
		  
		</div>
		

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2017/04/10/spark-streaming-02/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">Spark源码-streaming-02-接收数据的可靠性</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2017/04/03/spark-streaming-00/">
        <span class="next-text nav-default">Spark源码-streaming-00-ReceiverTracker 数据接收</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>

      </div>

      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2016 -
    
    2019
    <span class="footer-author">acyouzi.</span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
